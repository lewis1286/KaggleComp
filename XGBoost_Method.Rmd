---
title: "XGBoost_method"
author: "Lewis Guignard"
date: "November 3, 2015"
output: html_document
---


FYI, XGBoost wins most regression Kaggle competitions

# 1) Load the data
For part 1, we are supposed to train on training and validation, then test on leaderboard.

```{r}
rm(list=ls())

# Read in each data files into a data frame
training.target <- read.csv("../training_target.csv")
training.features <- read.csv("../training_features.csv")
validation.features <- read.csv("../validation_features.csv")
validation.target <- read.csv("../validation_target.csv")
leaderboard.features<- read.csv("../leaderboard_features.csv")
```



# 2) Handling NAs
We must remove some features: validation.features and leaderboard.features have many indicators that are all NA values.  We first set the NA values to the median of that column for the rest of the predictors.  Then we find the columns that are all NA, and remove them completely from training.features, validation.features, leaderboard.features.  

Subset features (predictors)
```{r}

for (i in seq(.2,.8, .1)){
  

ratio = .5
npred = nrow(training.features)
na.count <- apply(training.features,2,function(x){sum(is.na(x)) < ratio*npred})

t_feat = training.features[,na.count]
v_feat = validation.features[,na.count]
l_feat = leaderboard.features[,na.count]

# set NA's to medians for columns that have at least some data
t_feat <- as.data.frame(lapply(training.features, function(x) {x[is.na(x)] <- median(x, na.rm=TRUE); x}))
v_feat <- as.data.frame(lapply(validation.features, function(x) {x[is.na(x)] <- median(x, na.rm=TRUE); x}))
l_feat <- as.data.frame(lapply(leaderboard.features, function(x) {x[is.na(x)] <- median(x, na.rm=TRUE); x}))

# lets merge t_feat and v_feat for a training set
train.predictors <- rbind(t_feat, v_feat)
train.target <- rbind(training.target, validation.target)
train.frame <- data.frame(train.target, train.predictors)
train.frame <- train.frame[,-3] # subject id got copied in 2x, remove one of them

```


# 3) 

Most of this train / test is modified from:

https://github.com/dmlc/xgboost/blob/master/R-package/demo/basic_walkthrough.R
https://github.com/dmlc/xgboost/blob/master/R-package/R/xgboost.R

```{r}
require(xgboost)
require(boot)
# get rid of subject id while retaining ordering
train.target<-train.target$ALSFRS_slope
train.predictors<-train.predictors[,-1]
test.data <- v_feat[,-1] #l_feat[,-1]  # use l_feat[,-1] when you want to build a prediction for submission.

```

Train using XGBoost (I have no clue what these settings are)
```{r}

bst <- xgboost(data = as.matrix(train.predictors), label = train.target, max.depth = 3, eta = .8, nthread = 2, nround = 2, objective = "reg:linear")
```

Prediction:
```{r}
# you can put in Matrix, sparseMatrix, or xgb.DMatrix 
pred <- predict(bst, as.matrix(test.data))
mean(( pred - validation.target$ALSFRS_slope)^2)
#This result uses a test set included in the training set.
```


Run bootstrap on mean to see how it holds up to variations in the dataset.

```{r}
boot.fn = function(df, index){
  bst <- xgboost(data = as.matrix(df[index,]), label = train.target[index], max.depth = 2, eta = .9, nthread = 3, nround = 3, objective = "reg:linear")
  pred <- predict(bst, as.matrix(test.data))
  return(mean((pred - validation.target$ALSFRS_slope) ^2) )
}
boot(train.predictors, boot.fn, 50)
# boot.fn(train.predictors, sample(1500,1500, replace = TRUE))
```

```{r}
# get into format for submission
leaderboard.predictions <- data.frame(leaderboard.features$subject.id, pred)
names(leaderboard.predictions) = c("subject.id", "ALSFRS_slope")
```


Submission file:
We use **write.csv** function to write a CSV file in the contest format with the leaderboard subject predictions. 

```{r}
write.csv(leaderboard.predictions, file = "../xgb_leaderboard_predictions.csv",row.names=FALSE)
```
