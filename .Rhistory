regfit.train <- regsubsets(Y~., train.df, nvmax = ncol(train.df) - 1)
# rss is un-normalized MSE, we access rss of best subsets through the summary() object
plot(summary(regfit.train)$rss*100, ylab = "MSE", xlab="p")
title("Training set MSE by number of indicators p")
which.min(summary(regfit.train)$rss)
train.mat <- model.matrix(Y~.,data = train.df, nmax = ncol(train.df) -1)
test.mat <- model.matrix(Y~., data = test.df)
# no predict function for regsubsets.  We predict ourselves, using the best subset generated
# on the training set (regfit.train)
val.errors = rep(NA, 20)
for(i in 1:20){
coefi <- coef(regfit.train, id=i) # pulls out the coefficients of the 'i' best predictors
pred <- test.mat[,names(coefi)] %*% coefi
val.errors[i] = mean((test.df$Y - pred)^2)
}
plot(val.errors)
which.min(val.errors)
set.seed(1)
rm(list = ls())
sigma = sample(seq(1,6), 20, replace = TRUE) #standard deviations
mu = sample(seq(1,10), 20, replace = TRUE) # means
eps = rnorm(1000) # irreducable error
beta = rnorm(20) * rbinom(10, size = 1, prob = .7) # normal, with 20% squashed to 0 from bernoulli dist.
X <- matrix(rnorm(1000,mean = mu[1], sd = sigma[1])) #initialize dataframe
for (i in seq(2,20)){
X <- cbind(X, rnorm(1000, mean = mu[i], sd = sigma[i]))
}
# names(X) <- paste("X", seq(1,20))
Y = X %*% beta + eps
names(Y) = "Y"
train.df <- data.frame(Y[1:100], X[1:100,])
names(train.df)[1] = "Y"
test.df <- data.frame(Y[101:1000], X[101:1000,])
names(test.df)[1] = "Y"
library(leaps)
regfit.train <- regsubsets(Y~., train.df, nvmax = ncol(train.df) - 1)
# rss is un-normalized MSE, we access rss of best subsets through the summary() object
plot(summary(regfit.train)$rss*100, ylab = "MSE", xlab="p")
title("Training set MSE by number of indicators p")
train.mat <- model.matrix(Y~.,data = train.df, nmax = ncol(train.df) -1)
test.mat <- model.matrix(Y~., data = test.df)
# no predict function for regsubsets.  We predict ourselves, using the best subset generated
# on the training set (regfit.train)
val.errors = rep(NA, 20)
for(i in 1:20){
coefi <- coef(regfit.train, id=i) # pulls out the coefficients of the 'i' best predictors
pred <- test.mat[,names(coefi)] %*% coefi
val.errors[i] = mean((test.df$Y - pred)^2)
}
plot(val.errors)
which.min(val.errors)
train.mat <- model.matrix(Y~.,data = train.df, nmax = ncol(train.df) -1)
test.mat <- model.matrix(Y~., data = test.df)
# no predict function for regsubsets.  We predict ourselves, using the best subset generated
# on the training set (regfit.train)
val.errors = rep(NA, 20)
for(i in 1:20){
coefi <- coef(regfit.train, id=i) # pulls out the coefficients of the 'i' best predictors
pred <- test.mat[,names(coefi)] %*% coefi
val.errors[i] = mean((test.df$Y - pred)^2)
}
plot(val.errors, xlab = "p", ylab = "MSE test")
title(paste("MSE on test set, min MSE when p = ", which.min(val.errors))
which.min(val.errors)
train.mat <- model.matrix(Y~.,data = train.df, nmax = ncol(train.df) -1)
test.mat <- model.matrix(Y~., data = test.df)
# no predict function for regsubsets.  We predict ourselves, using the best subset generated
# on the training set (regfit.train)
val.errors = rep(NA, 20)
for(i in 1:20){
coefi <- coef(regfit.train, id=i) # pulls out the coefficients of the 'i' best predictors
pred <- test.mat[,names(coefi)] %*% coefi
val.errors[i] = mean((test.df$Y - pred)^2)
}
plot(val.errors, xlab = "p", ylab = "MSE test")
title(paste("MSE on test set, min MSE when p = ", which.min(val.errors)))
which.min(val.errors)
train.mat <- model.matrix(Y~.,data = train.df, nmax = ncol(train.df) -1)
test.mat <- model.matrix(Y~., data = test.df)
# no predict function for regsubsets.  We predict ourselves, using the best subset generated
# on the training set (regfit.train)
val.errors = rep(NA, 20)
for(i in 1:20){
coefi <- coef(regfit.train, id=i) # pulls out the coefficients of the 'i' best predictors
pred <- test.mat[,names(coefi)] %*% coefi
val.errors[i] = mean((test.df$Y - pred)^2)
}
plot(val.errors, xlab = "p", ylab = "MSE test")
title(paste("test MSE, min MSE when p = ", which.min(val.errors)))
which.min(val.errors)
train.mat <- model.matrix(Y~.,data = train.df, nmax = ncol(train.df) -1)
test.mat <- model.matrix(Y~., data = test.df)
# no predict function for regsubsets.  We predict ourselves, using the best subset generated
# on the training set (regfit.train)
val.errors = rep(NA, 20)
for(i in 1:20){
coefi <- coef(regfit.train, id=i) # pulls out the coefficients of the 'i' best predictors
pred <- test.mat[,names(coefi)] %*% coefi
val.errors[i] = mean((test.df$Y - pred)^2)
}
plot(val.errors, xlab = "p", ylab = "MSE test")
title(paste("test MSE, min MSE when p = ", which.min(val.errors)))
points(val.errors(which.min(val.errors)), col = "red", cex = 2, pch = 20)
which.min(val.errors)
train.mat <- model.matrix(Y~.,data = train.df, nmax = ncol(train.df) -1)
test.mat <- model.matrix(Y~., data = test.df)
# no predict function for regsubsets.  We predict ourselves, using the best subset generated
# on the training set (regfit.train)
val.errors = rep(NA, 20)
for(i in 1:20){
coefi <- coef(regfit.train, id=i) # pulls out the coefficients of the 'i' best predictors
pred <- test.mat[,names(coefi)] %*% coefi
val.errors[i] = mean((test.df$Y - pred)^2)
}
plot(val.errors, xlab = "p", ylab = "MSE test")
title(paste("test MSE, min MSE when p = ", which.min(val.errors)))
points(val.errors[which.min(val.errors)], col = "red", cex = 2, pch = 20)
which.min(val.errors)
train.mat <- model.matrix(Y~.,data = train.df, nmax = ncol(train.df) -1)
test.mat <- model.matrix(Y~., data = test.df)
# no predict function for regsubsets.  We predict ourselves, using the best subset generated
# on the training set (regfit.train)
val.errors = rep(NA, 20)
for(i in 1:20){
coefi <- coef(regfit.train, id=i) # pulls out the coefficients of the 'i' best predictors
pred <- test.mat[,names(coefi)] %*% coefi
val.errors[i] = mean((test.df$Y - pred)^2)
}
plot(val.errors, xlab = "p", ylab = "MSE test")
title(paste("test MSE, min MSE when p = ", which.min(val.errors)))
points(which.min(val.errors),val.errors[which.min(val.errors)], col = "red", cex = 2, pch = 20)
which.min(val.errors)
train.mat <- model.matrix(Y~.,data = train.df, nmax = ncol(train.df) -1)
test.mat <- model.matrix(Y~., data = test.df)
# no predict function for regsubsets.  We predict ourselves, using the best subset generated
# on the training set (regfit.train)
val.errors = rep(NA, 20)
for(i in 1:20){
coefi <- coef(regfit.train, id=i) # pulls out the coefficients of the 'i' best predictors
pred <- test.mat[,names(coefi)] %*% coefi
val.errors[i] = mean((test.df$Y - pred)^2)
}
plot(val.errors, xlab = "p", ylab = "MSE test")
title(paste("test MSE, min MSE when p = ", which.min(val.errors)))
points(which.min(val.errors),val.errors[which.min(val.errors)],
col = "red", cex = 2, pch = 20)
which.min(val.errors)
beta
#set of beta estimates for minimum train MSE
hatbeta <- coef(regfit.train, id=which.min(val.errors))
beta - hatbeta
length(hatbeta)
length(beta)
#set of beta estimates for minimum train MSE
hatbeta <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
beta - hatbeta
length(hatbeta)
coef(regfit.train, id = 15)
beta
names(beta) = paste("X",rep(1:20))
beta
paste0("X",rep(1:20))
set.seed(1)
rm(list = ls())
sigma = sample(seq(1,6), 20, replace = TRUE) #standard deviations
mu = sample(seq(1,10), 20, replace = TRUE) # means
eps = rnorm(1000) # irreducable error
beta = rnorm(20) * rbinom(10, size = 1, prob = .7) # normal, with 20% squashed to 0 from bernoulli dist.
names(beta) <- paste0("X",seq(1:20))
X <- matrix(rnorm(1000,mean = mu[1], sd = sigma[1])) #initialize dataframe
for (i in seq(2,20)){
X <- cbind(X, rnorm(1000, mean = mu[i], sd = sigma[i]))
}
# names(X) <- paste("X", seq(1,20))
Y = X %*% beta + eps
names(Y) = "Y"
beta
hatbeta
#set of beta estimates for minimum train MSE
hatbeta <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
# beta - hatbeta
set.seed(1)
rm(list = ls())
sigma = sample(seq(1,6), 20, replace = TRUE) #standard deviations
mu = sample(seq(1,10), 20, replace = TRUE) # means
eps = rnorm(1000) # irreducable error
beta = rnorm(20) * rbinom(10, size = 1, prob = .7) # normal, with 20% squashed to 0 from bernoulli dist.
names(beta) <- paste0("X",seq(1:20))
X <- matrix(rnorm(1000,mean = mu[1], sd = sigma[1])) #initialize dataframe
for (i in seq(2,20)){
X <- cbind(X, rnorm(1000, mean = mu[i], sd = sigma[i]))
}
# names(X) <- paste("X", seq(1,20))
Y = X %*% beta + eps
names(Y) = "Y"
train.df <- data.frame(Y[1:100], X[1:100,])
names(train.df)[1] = "Y"
test.df <- data.frame(Y[101:1000], X[101:1000,])
names(test.df)[1] = "Y"
library(leaps)
regfit.train <- regsubsets(Y~., train.df, nvmax = ncol(train.df) - 1)
# rss is un-normalized MSE, we access rss of best subsets through the summary() object
plot(summary(regfit.train)$rss*100, ylab = "MSE", xlab="p")
title("Training set MSE by number of indicators p")
train.mat <- model.matrix(Y~.,data = train.df, nmax = ncol(train.df) -1)
test.mat <- model.matrix(Y~., data = test.df)
# no predict function for regsubsets.  We predict ourselves, using the best subset generated
# on the training set (regfit.train)
val.errors = rep(NA, 20)
for(i in 1:20){
coefi <- coef(regfit.train, id=i) # pulls out the coefficients of the 'i' best predictors
pred <- test.mat[,names(coefi)] %*% coefi
val.errors[i] = mean((test.df$Y - pred)^2)
}
plot(val.errors, xlab = "p", ylab = "MSE test")
title(paste("test MSE, min MSE when p = ", which.min(val.errors)))
points(which.min(val.errors),val.errors[which.min(val.errors)],
col = "red", cex = 2, pch = 20)
which.min(val.errors)
#set of beta estimates for minimum train MSE
hatbeta <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
# beta - hatbeta
hatbeta
#set of beta estimates for minimum train MSE
hatbeta <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
beta.subset <- beta[names(coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]]
names(coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
#set of beta estimates for minimum train MSE
hatbeta <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
beta.subset <- beta[names(coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)])]
names(coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)])
beta[names(coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)])]
#set of beta estimates for minimum train MSE
hatbeta <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
beta.subset <- beta[names(coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)])]
hatbeta - beta.subset
# set of beta estimates for minimum train MSE
hat.beta <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
# subset of original beta corresponding to best p predictors
beta.subset <- beta[names(coef(regfit.train,
id=which.min(val.errors))[2:which.min(val.errors)])]
?norm
norm(hat.beta - beta.subset)
# set of beta estimates for minimum train MSE
beta.hat <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
# subset of original beta corresponding to best p predictors from test set
beta.subset <- beta[names(coef(regfit.train,
id=which.min(val.errors))[2:which.min(val.errors)])]
beta.hat - beta.subset
# set of beta estimates for minimum train MSE
beta.hat <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
# subset of original beta corresponding to best p predictors from test set
beta.subset <- beta[names(coef(regfit.train,
id=which.min(val.errors))[2:which.min(val.errors)])]
(beta.hat - beta.subset)/beta.subset
# set of beta estimates for minimum train MSE
beta.hat <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
# subset of original beta corresponding to best p predictors from test set
beta.subset <- beta[names(coef(regfit.train,
id=which.min(val.errors))[2:which.min(val.errors)])]
abs((beta.hat - beta.subset)/beta.subset)
# set of beta estimates for minimum train MSE
beta.hat <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
# subset of original beta corresponding to best p predictors from test set
beta.subset <- beta[names(coef(regfit.train,
id=which.min(val.errors))[2:which.min(val.errors)])]
max(abs((beta.hat - beta.subset)/beta.subset))
beta
# set of beta estimates for minimum train MSE
beta.hat <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
# subset of original beta corresponding to best p predictors from test set
beta.subset <- beta[names(coef(regfit.train,
id=which.min(val.errors))[2:which.min(val.errors)])]
abs((beta.hat - beta.subset)/beta.subset)
seq(1:10)
sqrt(4)
coef(regfit.train, id=1)
coef(regfit.train, id=20)
coef(regfit.train, id=1)
coef(regfit.train, id=1)[2:2]
beta.subset
# set of beta estimates for minimum train MSE
beta.hat <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
# subset of original beta corresponding to best p predictors from test set
beta.subset <- beta[names(coef(regfit.train,
id=which.min(val.errors))#[2:which.min(val.errors)])]
abs((beta.hat - beta.subset)/beta.subset)
# set of beta estimates for minimum train MSE
beta.hat <- coef(regfit.train, id=which.min(val.errors))[2:which.min(val.errors)]
# subset of original beta corresponding to best p predictors from test set
beta.subset <- beta[names(beta.hat)]
abs((beta.hat - beta.subset)/beta.subset)
r = 5
coef(regfit.train, id=r)[2:r + 1]
coef(regfit.train, id=r)[1:r + 1]
coef(regfit.train, id=5)
coef(regfit.train, id=r)[2:r + 1]
coef(regfit.train, id=5)[2:5 + 1]
coef(regfit.train, id=5)[1:5 + 1]
x = 1:5
x
x[1:5]
coef(regfit.train, id = 5)
coef(regfit.train, id = 5)[2:6]
coef(regfit.train, id = 5)[1:6]
for (r in seq(1:4)){
beta.hat <- coef(regfit.train, id=r)[2:r + 1]
beta.hat
beta.subset beta[names(beta.hat)]
beta.subset
}
for (r in seq(1:4)){
beta.hat <- coef(regfit.train, id=r)[2:r + 1]
beta.hat
beta.subset <- beta[names(beta.hat)]
beta.subset
}
for (r in seq(1:4)){
beta.hat <- coef(regfit.train, id=r)[2:r + 1]
print(beta.hat)
beta.subset <- beta[names(beta.hat)]
beta.subset
}
for (r in seq(1:4)){
beta.hat <- coef(regfit.train, id=r)[2:r + 1]
print(beta.hat)
beta.subset <- beta[names(beta.hat)]
print(beta.subset)
}
for (r in seq(2:4)){
beta.hat <- coef(regfit.train, id=r)[2:r + 1]
print(beta.hat)
beta.subset <- beta[names(beta.hat)]
print(beta.subset)
}
for (r in seq(1:4)){
beta.hat <- coef(regfit.train, id=r)[2:r + 1]
print(beta.hat)
beta.subset <- beta[names(beta.hat)]
print(beta.subset)
}
for (r in seq(1:5)){
beta.hat <- coef(regfit.train, id=r)[2:r + 1]
print(beta.hat)
beta.subset <- beta[names(beta.hat)]
print(beta.subset)
}
val = rep(0,20)
for (r in seq(1:5)){
beta.hat <- coef(regfit.train, id=r)[2:r + 1]
# print(beta.hat)
beta.subset <- beta[names(beta.hat)]
# print(beta.subset)
val[r] <- sqrt(sum((beta.hat - beta.subset)^2))
}
val = rep(0,20)
for (r in seq(1:5)){
beta.hat <- coef(regfit.train, id=r)[2:r + 1]
# print(beta.hat)
beta.subset <- beta[names(beta.hat)]
# print(beta.subset)
val[r] <- sqrt(sum((beta.hat - beta.subset)^2))
}
plot(seq(1:20), val)
val
val = rep(0,20)
for (r in seq(1:20)){
beta.hat <- coef(regfit.train, id=r)[2:r + 1]
# print(beta.hat)
beta.subset <- beta[names(beta.hat)]
# print(beta.subset)
val[r] <- sqrt(sum((beta.hat - beta.subset)^2))
}
plot(seq(1:20), val)
val
val[15:16]
val = rep(0,20)
for (r in seq(1:20)){
beta.hat <- coef(regfit.train, id=r)[2:r + 1]
# print(beta.hat)
beta.subset <- beta[names(beta.hat)]
# print(beta.subset)
if(r == 1){
beta.hat <- beta.hat[2]
beta.subset <- beta.subset[2]
}
val[r] <- sqrt(sum((beta.hat - beta.subset)^2))
}
plot(seq(1:20), val)
train.mat <- model.matrix(Y~.,data = train.df, nmax = ncol(train.df) -1)
test.mat <- model.matrix(Y~., data = test.df)
# no predict function for regsubsets.  We predict ourselves, using the best subset generated
# on the training set (regfit.train)
val.errors = rep(NA, 20)
for(i in 1:20){
coefi <- coef(regfit.train, id=i) # pulls out the coefficients of the 'i' best predictors
pred <- test.mat[,names(coefi)] %*% coefi
val.errors[i] = mean((test.df$Y - pred)^2)
}
plot(val.errors, xlab = "p", ylab = "MSE test")
title(paste("test MSE, min MSE when p = ", which.min(val.errors)))
points(which.min(val.errors),val.errors[which.min(val.errors)],
col = "red", cex = 2, pch = 20)
which.min(val.errors)
boot.fn = function(df, index){
bst <- xgboost(data = as.matrix(df[index,]), label = train.target[index], max.depth = 2, eta = .9, nthread = 3, nround = 3, objective = "reg:linear")
pred <- predict(bst, as.matrix(test.data))
return(mean((pred - validation.target$ALSFRS_slope) ^2) )
}
boot(train.predictors, boot.fn, 50)
# boot.fn(train.predictors, sample(1500,1500, replace = TRUE))
require(boot)
boot.fn = function(df, index){
bst <- xgboost(data = as.matrix(df[index,]), label = train.target[index], max.depth = 2, eta = .9, nthread = 3, nround = 3, objective = "reg:linear")
pred <- predict(bst, as.matrix(test.data))
return(mean((pred - validation.target$ALSFRS_slope) ^2) )
}
boot(train.predictors, boot.fn, 50)
# boot.fn(train.predictors, sample(1500,1500, replace = TRUE))
rm(list=ls())
# Read in each data files into a data frame
training.target <- read.csv("../training_target.csv")
training.features <- read.csv("../training_features.csv")
validation.features <- read.csv("../validation_features.csv")
validation.target <- read.csv("../validation_target.csv")
leaderboard.features<- read.csv("../leaderboard_features.csv")
# set NA's to medians for columns that have at least some data
t_feat <- as.data.frame(lapply(training.features, function(x) {x[is.na(x)] <- median(x, na.rm=TRUE); x}))
v_feat <- as.data.frame(lapply(validation.features, function(x) {x[is.na(x)] <- median(x, na.rm=TRUE); x}))
l_feat <- as.data.frame(lapply(leaderboard.features, function(x) {x[is.na(x)] <- median(x, na.rm=TRUE); x}))
# find out which columns in v_feat are all NA, and remove these columns from t_feat, v_feat, l_feat
col.has.na <- apply(l_feat, 2, function(x){any(is.na(x))}) # logical of which columns are NA
t_feat <- t_feat[,!col.has.na]
v_feat <- v_feat[,!col.has.na]
l_feat <- l_feat[,!col.has.na]
# lets merge t_feat and v_feat for a training set
train.predictors <- rbind(t_feat, v_feat)
train.target <- rbind(training.target, validation.target)
train.frame <- data.frame(train.target, train.predictors)
train.frame <- train.frame[,-3] # subject id got copied in 2x, remove one of them
require(xgboost)
require(boot)
# get rid of subject id while retaining ordering
train.target<-train.target$ALSFRS_slope
train.predictors<-train.predictors[,-1]
test.data <- v_feat[,-1] #l_feat[,-1]  # use l_feat[,-1] when you want to build a prediction for submission.
boot.fn = function(df, index){
bst <- xgboost(data = as.matrix(df[index,]), label = train.target[index], max.depth = 2, eta = .9, nthread = 3, nround = 3, objective = "reg:linear")
pred <- predict(bst, as.matrix(test.data))
return(mean((pred - validation.target$ALSFRS_slope) ^2) )
}
boot(train.predictors, boot.fn, 50)
# boot.fn(train.predictors, sample(1500,1500, replace = TRUE))
# set NA's to medians for columns that have at least some data
t_feat <- as.data.frame(lapply(training.features, function(x) {x[is.na(x)] <- median(x, na.rm=TRUE); x}))
v_feat <- as.data.frame(lapply(validation.features, function(x) {x[is.na(x)] <- median(x, na.rm=TRUE); x}))
l_feat <- as.data.frame(lapply(leaderboard.features, function(x) {x[is.na(x)] <- median(x, na.rm=TRUE); x}))
# find out which columns in v_feat are all NA, and remove these columns from t_feat, v_feat, l_feat
col.has.na <- apply(l_feat, 2, function(x){any(is.na(x))}) # logical of which columns are NA
t_feat <- t_feat[,!col.has.na]
v_feat <- v_feat[,!col.has.na]
l_feat <- l_feat[,!col.has.na]
# lets merge t_feat and v_feat for a training set
train.predictors <- rbind(t_feat, v_feat)
train.target <- rbind(training.target, validation.target)
train.frame <- data.frame(train.target, train.predictors)
train.frame <- train.frame[,-3] # subject id got copied in 2x, remove one of them
setwd("~/Dropbox/classes/STATs 202 data mining and analysis/ALS-kaggle-comp/KaggleComp")
rm(list=ls())
# Read in each data files into a data frame
training.target <- read.csv("../training_target.csv")
training.features <- read.csv("../training_features.csv")
validation.features <- read.csv("../validation_features.csv")
validation.target <- read.csv("../validation_target.csv")
leaderboard.features<- read.csv("../leaderboard_features.csv")
# set NA's to medians for columns that have at least some data
t_feat <- as.data.frame(lapply(training.features, function(x) {x[is.na(x)] <- median(x, na.rm=TRUE); x}))
v_feat <- as.data.frame(lapply(validation.features, function(x) {x[is.na(x)] <- median(x, na.rm=TRUE); x}))
l_feat <- as.data.frame(lapply(leaderboard.features, function(x) {x[is.na(x)] <- median(x, na.rm=TRUE); x}))
# find out which columns in v_feat are all NA, and remove these columns from t_feat, v_feat, l_feat
col.has.na <- apply(l_feat, 2, function(x){any(is.na(x))}) # logical of which columns are NA
t_feat <- t_feat[,!col.has.na]
v_feat <- v_feat[,!col.has.na]
l_feat <- l_feat[,!col.has.na]
# lets merge t_feat and v_feat for a training set
train.predictors <- rbind(t_feat, v_feat)
train.target <- rbind(training.target, validation.target)
train.frame <- data.frame(train.target, train.predictors)
train.frame <- train.frame[,-3] # subject id got copied in 2x, remove one of them
require(xgboost)
require(boot)
# get rid of subject id while retaining ordering
train.target<-train.target$ALSFRS_slope
train.predictors<-train.predictors[,-1]
test.data <- v_feat[,-1] #l_feat[,-1]  # use l_feat[,-1] when you want to build a prediction for submission.
boot.fn = function(df, index){
bst <- xgboost(data = as.matrix(df[index,]), label = train.target[index], max.depth = 2, eta = .9, nthread = 3, nround = 3, objective = "reg:linear")
pred <- predict(bst, as.matrix(test.data))
return(mean((pred - validation.target$ALSFRS_slope) ^2) )
}
boot(train.predictors, boot.fn, 50)
# boot.fn(train.predictors, sample(1500,1500, replace = TRUE))
rm(list=ls())
# Read in each data files into a data frame
training.target <- read.csv("../training_target.csv")
training.features <- read.csv("../training_features.csv")
validation.features <- read.csv("../validation_features.csv")
validation.target <- read.csv("../validation_target.csv")
leaderboard.features<- read.csv("../leaderboard_features.csv")
View(leaderboard.features)
View(leaderboard.features)
View(validation.target)
